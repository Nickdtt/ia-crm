"""
LLM Factory - AbstraÃ§Ã£o centralizada para mÃºltiplos providers de LLM.

Este mÃ³dulo implementa o Factory Pattern para gerenciar diferentes providers
de LLM de forma centralizada, permitindo trocar entre providers apenas
alterando variÃ¡veis no arquivo .env.

Providers Suportados:
    - groq: ChatGroq (padrÃ£o, rÃ¡pido, gratuito, remoto)
    - ollama: ChatOllama (local, privado, sem custo de API)
    - openai: ChatOpenAI (GPT-4, etc)
    - anthropic: ChatAnthropic (Claude)

Uso:
    from app.core.llm_factory import get_llm
    
    # Usa automaticamente o provider configurado no .env
    llm = get_llm()
    response = await llm.ainvoke(messages)

ConfiguraÃ§Ã£o (.env):
    # Para usar Groq (padrÃ£o)
    DEFAULT_LLM_PROVIDER=groq
    DEFAULT_MODEL=llama-3.3-70b-versatile
    GROQ_API_KEY=gsk_...
    
    # Para usar Ollama local
    DEFAULT_LLM_PROVIDER=ollama
    OLLAMA_MODEL=llama3.1:8b
    OLLAMA_BASE_URL=http://localhost:11434
"""

from langchain_core.language_models import BaseChatModel
from app.core.config import settings


def get_llm() -> BaseChatModel:
    """
    Factory que retorna o LLM configurado baseado em DEFAULT_LLM_PROVIDER.
    
    Esta funÃ§Ã£o centraliza a criaÃ§Ã£o de instÃ¢ncias de LLM, eliminando
    duplicaÃ§Ã£o de cÃ³digo e permitindo trocar de provider facilmente.
    
    O provider Ã© determinado pela variÃ¡vel DEFAULT_LLM_PROVIDER no .env.
    
    Returns:
        BaseChatModel: InstÃ¢ncia do LLM configurado (ChatGroq, ChatOllama, etc)
        
    Raises:
        ValueError: Se o provider nÃ£o Ã© suportado ou credenciais estÃ£o faltando
        
    Examples:
        >>> llm = get_llm()  # Usa DEFAULT_LLM_PROVIDER do .env
        >>> response = await llm.ainvoke([HumanMessage(content="OlÃ¡")])
    """
    
    provider = settings.DEFAULT_LLM_PROVIDER.lower()
    
    # ðŸš€ GROQ - RÃ¡pido, gratuito, remoto (padrÃ£o)
    if provider == "groq":
        if not settings.GROQ_API_KEY:
            raise ValueError(
                "GROQ_API_KEY nÃ£o encontrada no .env. "
                "Adicione: GROQ_API_KEY=gsk_..."
            )
        
        from langchain_groq import ChatGroq
        
        return ChatGroq(
            model=settings.DEFAULT_MODEL,
            temperature=settings.TEMPERATURE,
            groq_api_key=settings.GROQ_API_KEY,
            max_retries=3,
            timeout=30,
        )
    
    # ðŸ¦™ OLLAMA - Local, privado, sem custo de API
    elif provider == "ollama":
        from langchain_ollama import ChatOllama
        
        print(f"ðŸ¦™ Usando Ollama local: {settings.OLLAMA_BASE_URL}")
        print(f"ðŸ“¦ Modelo: {settings.OLLAMA_MODEL}")
        
        return ChatOllama(
            model=settings.OLLAMA_MODEL,
            temperature=settings.TEMPERATURE,
            base_url=settings.OLLAMA_BASE_URL
        )
    
    # ðŸ¤– OPENAI - GPT-4, GPT-3.5, etc
    elif provider == "openai":
        if not settings.OPENAI_API_KEY:
            raise ValueError(
                "OPENAI_API_KEY nÃ£o encontrada no .env. "
                "Adicione: OPENAI_API_KEY=sk-..."
            )
        
        from langchain_openai import ChatOpenAI
        
        return ChatOpenAI(
            model=settings.DEFAULT_MODEL,
            temperature=settings.TEMPERATURE,
            api_key=settings.OPENAI_API_KEY
        )
    
    # ðŸ§  ANTHROPIC - Claude 3, etc
    elif provider == "anthropic":
        if not settings.ANTHROPIC_API_KEY:
            raise ValueError(
                "ANTHROPIC_API_KEY nÃ£o encontrada no .env. "
                "Adicione: ANTHROPIC_API_KEY=sk-ant-..."
            )
        
        from langchain_anthropic import ChatAnthropic
        
        return ChatAnthropic(
            model=settings.DEFAULT_MODEL,
            temperature=settings.TEMPERATURE,
            api_key=settings.ANTHROPIC_API_KEY
        )
    
    else:
        raise ValueError(
            f"Provider '{provider}' nÃ£o suportado. "
            f"Use: 'groq', 'ollama', 'openai', ou 'anthropic'"
        )


def get_llm_info() -> dict:
    """
    Retorna informaÃ§Ãµes sobre o LLM atual configurado.
    
    Ãštil para debugging, logs e verificaÃ§Ã£o de configuraÃ§Ã£o.
    
    Returns:
        dict: DicionÃ¡rio com provider, modelo, temperatura, etc
        
    Examples:
        >>> info = get_llm_info()
        >>> print(f"Usando {info['provider']} com {info['model']}")
        Usando groq com llama-3.3-70b-versatile
    """
    
    provider = settings.DEFAULT_LLM_PROVIDER.lower()
    
    info = {
        "provider": provider,
        "temperature": settings.TEMPERATURE,
    }
    
    if provider == "groq":
        info["model"] = settings.DEFAULT_MODEL
        info["api_key_set"] = bool(settings.GROQ_API_KEY)
        
    elif provider == "ollama":
        info["model"] = settings.OLLAMA_MODEL
        info["base_url"] = settings.OLLAMA_BASE_URL
        
    elif provider == "openai":
        info["model"] = settings.DEFAULT_MODEL
        info["api_key_set"] = bool(settings.OPENAI_API_KEY)
        
    elif provider == "anthropic":
        info["model"] = settings.DEFAULT_MODEL
        info["api_key_set"] = bool(settings.ANTHROPIC_API_KEY)
    
    return info
