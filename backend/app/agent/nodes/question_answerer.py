import re

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

from app.core.llm_factory import get_llm
from app.agent.state import MarketingCRMState
from app.rag import search_documents


QUESTION_ANSWERER_PROMPT = """Voc√™ √© o agente virtual da "Isso n√£o √© uma ag√™ncia", est√∫dio de crescimento digital.

O cliente fez uma pergunta. Use o CONTEXTO abaixo (extra√≠do dos documentos da empresa) para responder com precis√£o.

REGRAS:
- Responda de forma OBJETIVA (m√°ximo 3-4 linhas)
- Use APENAS informa√ß√µes do contexto fornecido ‚Äî n√£o invente dados
- Se o contexto n√£o contiver a resposta, diga que pode verificar e ofere√ßa a consultoria gratuita
- Pode mencionar valores e planos SE estiverem no contexto
- Pode citar cases de sucesso SE estiverem no contexto
- SEM pressionar, seja consultivo e natural

{permission_instruction}

CONTEXTO DOS DOCUMENTOS:
{rag_context}"""


# ========== HELPERS DE DETEC√á√ÉO ==========

EMAIL_REGEX = re.compile(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+')


def _detect_email(text: str) -> str | None:
    """Detecta email via regex."""
    match = EMAIL_REGEX.search(text)
    return match.group(0).lower() if match else None


def _detect_name(text: str) -> str | None:
    """Detecta nome completo (2+ palavras alfab√©ticas).
    
    Exclui respostas comuns que n√£o s√£o nomes.
    """
    cleaned = text.strip().lower()
    
    # Rejeita se cont√©m caracteres especiais ou n√∫meros
    if '?' in cleaned or '@' in cleaned or any(c.isdigit() for c in cleaned):
        return None
    
    # Palavras que indicam N√ÉO √© nome (respostas comuns)
    reject_patterns = [
        'n√£o', 'nao', 'agora n√£o', 'agora nao', 'ainda n√£o', 'ainda nao',
        'depois', 'talvez', 'sem tempo', 'mais tarde', 'tenho pressa',
        's√≥ quero', 'quero saber', 'me fala', 'pode ser', 'obrigado',
        'por favor', 'com certeza', 'claro', 'sim', 'ok'
    ]
    
    if any(pattern in cleaned for pattern in reject_patterns):
        return None
    
    words = text.strip().split()
    
    # 2-5 palavras, todas alfab√©ticas (permite acentos)
    if 2 <= len(words) <= 5 and all(re.match(r'^[a-zA-Z√Ä-√ø]+$', w) for w in words):
        return text.strip().title()
    
    return None


async def question_answerer_node(state: MarketingCRMState) -> dict:
    """NODE: QUESTION_ANSWERER - Responde pergunta com RAG.
    
    MODO 1: Primeira vez (permission_asked=False) ‚Üí responde pergunta
    MODO 2: J√° respondeu ‚Üí analisa resposta:
        - Se detecta inten√ß√£o de agendamento ‚Üí roteia para lead collection
        - Dados forn./aceitou ‚Üí seta permission_granted + extrai dados + roteia
        - Recusou ‚Üí responde com RAG SEM re-pedir permiss√£o
        - Nova pergunta ‚Üí responde com RAG
    """
    
    last_message = state["messages"][-1]
    user_input = last_message.content if hasattr(last_message, 'content') else str(last_message)
    permission_asked = state.get("permission_asked", False)
    permission_granted = state.get("permission_granted")
    llm = get_llm()
    
    # --- Detec√ß√£o de inten√ß√£o de agendamento via LLM ---
    intent_prompt = f"""Analise esta mensagem do usu√°rio:

"{user_input}"

O usu√°rio quer agendar/marcar uma reuni√£o/consultoria?
Palavras-chave: "quero", "agendar", "marcar", "reuni√£o", "consultoria", etc.

Responda APENAS: SIM ou NAO"""

    try:
        intent_response = await llm.ainvoke([HumanMessage(content=intent_prompt)])
        has_scheduling_intent = "sim" in intent_response.content.strip().lower()
    except:
        has_scheduling_intent = False
    
    if has_scheduling_intent:
        print(f"üéØ Question Answerer: Detectou inten√ß√£o de agendamento ‚Üí indo para lead collection")
        return {
            "permission_granted": True,
            "messages": [AIMessage(content="Perfeito! Vou te ajudar com isso. Primeiro, qual √© o seu nome completo?")],
            "current_step": "lead_collector"
        }
    
    # MODO 2: J√° pediu permiss√£o ‚Äî analisar resposta do usu√°rio
    if permission_asked and permission_granted is None:
        
        # --- Detec√ß√£o r√°pida de email (regex √© confi√°vel aqui) ---
        detected_email = _detect_email(user_input)
        
        # --- Detec√ß√£o de nome via LLM (mais confi√°vel) ---
        detected_name = None
        if not detected_email and len(user_input.split()) <= 5:
            name_prompt = f"""A mensagem abaixo √© um NOME COMPLETO de pessoa?

"{user_input}"

Responda:
- Se for nome completo (2+ palavras): SIM|Nome Formatado
- Se n√£o for nome: NAO

Exemplos:
- "Nicolas Figueiredo" ‚Üí SIM|Nicolas Figueiredo
- "me conte" ‚Üí NAO
- "quero reuniao" ‚Üí NAO"""

            try:
                name_response = await llm.ainvoke([HumanMessage(content=name_prompt)])
                result = name_response.content.strip()
                if result.startswith("SIM|"):
                    detected_name = result.split("|")[1].strip()
                    print(f"‚úÖ Question Answerer: Nome detectado via LLM '{detected_name}'")
            except:
                pass
        
        if detected_name:
            # Usu√°rio j√° mandou o nome ‚Üí salvar + confirmar + pedir email
            return {
                "permission_granted": True,
                "lead_name": detected_name,
                "messages": [AIMessage(content=f"Prazer, {detected_name.split()[0]}! Qual √© o seu email?")],
                "current_step": "lead_collector"
            }
        
        if detected_email:
            # Usu√°rio mandou email ‚Üí salvar + confirmar + pedir nome
            return {
                "permission_granted": True,
                "lead_email": detected_email,
                "messages": [AIMessage(content=f"Anotei! Qual √© o seu nome completo?")],
                "current_step": "lead_collector"
            }
        
        # --- Detec√ß√£o via LLM: aceite expl√≠cito ---
        
        analysis_prompt = f"""Analise esta resposta do usu√°rio:

Mensagem: "{user_input}"

Contexto: Perguntamos se pod√≠amos fazer algumas perguntas para entender o neg√≥cio dele.

O usu√°rio ACEITOU? ("sim", "pode", "claro", "ok", "vamos l√°" = SIM)
Ou RECUSOU / fez outra pergunta? ("n√£o", "agora n√£o", pergunta nova = NAO)

Responda APENAS: SIM ou NAO"""
        
        analysis = await llm.ainvoke([HumanMessage(content=analysis_prompt)])
        accepted = "sim" in analysis.content.strip().lower()
        
        if accepted:
            print(f"‚úÖ Question Answerer: Usu√°rio ACEITOU explicitamente")
            return {
                "permission_granted": True,
                "messages": [AIMessage(content="√ìtimo! Vamos l√° ent√£o. Qual √© o seu nome completo?")],
                "current_step": "lead_collector"
            }
        
        # --- Recusou ou fez outra pergunta ‚Üí responder com RAG SEM re-pedir ---
        print(f"‚ÑπÔ∏è Question Answerer: Usu√°rio RECUSOU ou fez nova pergunta ‚Üí respondendo SEM insistir")
        
        rag_results = search_documents(user_input, top_k=3)
        rag_context = "\n\n---\n\n".join(rag_results) if rag_results else "(Nenhum documento relevante encontrado)"
        
        prompt = QUESTION_ANSWERER_PROMPT.format(
            rag_context=rag_context,
            permission_instruction="Responda apenas a pergunta. N√ÉO pe√ßa permiss√£o para fazer perguntas. Seja prestativo e deixe o cliente no controle da conversa."
        )
        
        messages = [
            SystemMessage(content=prompt),
            HumanMessage(content=f'Pergunta do cliente: "{user_input}"\n\nSua resposta:')
        ]
        
        try:
            response = await llm.ainvoke(messages)
            answer = response.content
        except Exception as e:
            print(f"‚ùå Erro: {type(e).__name__}: {e}")
            answer = "Entendo. Estou aqui se precisar de mais alguma informa√ß√£o! üòä"
        
        return {
            "messages": [AIMessage(content=answer)],
            "current_step": "question_answerer"
        }
    
    # MODO 1: Primeira resposta ‚Äî RAG simples (sem pedir permiss√£o automaticamente)
    rag_results = search_documents(user_input, top_k=3)
    
    if rag_results:
        rag_context = "\n\n---\n\n".join(rag_results)
        print(f"üìö RAG: {len(rag_results)} chunks encontrados para: '{user_input[:50]}...'")
    else:
        rag_context = "(Nenhum documento relevante encontrado)"
        print(f"üìö RAG: nenhum resultado para: '{user_input[:50]}...'")
    
    prompt = QUESTION_ANSWERER_PROMPT.format(
        rag_context=rag_context,
        permission_instruction="Responda a pergunta de forma objetiva e prestativa. Seja dispon√≠vel mas n√£o insistente. Se o cliente demonstrar interesse em saber mais ou agendar algo, a√≠ sim voc√™ pode se oferecer para ajudar."
    )
    
    llm = get_llm()
    
    messages = [
        SystemMessage(content=prompt),
        HumanMessage(content=f'Pergunta do cliente: "{user_input}"\n\nSua resposta:')
    ]
    
    try:
        response = await llm.ainvoke(messages)
        answer = response.content
    except Exception as e:
        print(f"‚ùå Erro ao responder pergunta com LLM: {type(e).__name__}: {e}")
        answer = "N√≥s constru√≠mos sistemas de aquisi√ß√£o de clientes ‚Äî n√£o somos uma ag√™ncia tradicional. Posso te contar mais sobre como funcionamos! üòä"
    
    print(f"üí¨ Question Answerer: Respondeu com RAG (sem pressionar)")
    
    return {
        "messages": [AIMessage(content=answer)],
        "permission_asked": True,
        "conversation_mode": "qualification",
        "current_step": "question_answerer"
    }
